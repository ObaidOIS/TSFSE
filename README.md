# ğŸ“° Bloomberg News Scraper & AI-Powered Search Engine

<div align="center">

![Python](https://img.shields.io/badge/Python-3.11+-blue?style=for-the-badge&logo=python)
![Django](https://img.shields.io/badge/Django-5.0-green?style=for-the-badge&logo=django)
![Next.js](https://img.shields.io/badge/Next.js-14-black?style=for-the-badge&logo=next.js)
![TypeScript](https://img.shields.io/badge/TypeScript-5.3-blue?style=for-the-badge&logo=typescript)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-16-blue?style=for-the-badge&logo=postgresql)
![Redis](https://img.shields.io/badge/Redis-7-red?style=for-the-badge&logo=redis)
![Docker](https://img.shields.io/badge/Docker-Ready-blue?style=for-the-badge&logo=docker)

**A full-stack news aggregation platform with AI-powered categorization and intelligent search**

[Features](#-features) â€¢ [Tech Stack](#-tech-stack) â€¢ [Quick Start](#-quick-start) â€¢ [API Reference](#-api-reference) â€¢ [Architecture](#-architecture)

</div>

---

## ğŸŒŸ Features

### ğŸ“Š Intelligent News Aggregation
- **Automated Bloomberg Scraping**: RSS feed parsing with intelligent content extraction
- **Real-time Updates**: Celery-based periodic scraping (every 5 minutes)
- **Duplicate Detection**: Smart deduplication using URL hashing and content similarity
- **Change Detection**: Monitors article updates and tracks content changes

### ğŸ¤– AI-Powered Categorization
- **Lightweight Keyword-Based AI**: Fast, efficient categorization without heavy ML dependencies
- **Multi-Category Support**: Economy, Market, Health, Technology, Industry, and more
- **Confidence Scoring**: Each article includes AI confidence percentage
- **Keyword Extraction**: Intelligent keyword and entity extraction from article content

### ğŸ” Advanced Search Engine
- **Full-Text Search**: PostgreSQL tsvector for fast, accurate searches
- **Auto-Category Detection**: AI automatically detects category from search queries
- **Relevance Ranking**: BM25-inspired scoring with multiple ranking factors
- **Real-time Suggestions**: Search autocomplete with category predictions

### ğŸ¨ Modern Frontend
- **Next.js 14 App Router**: Latest React Server Components
- **Responsive Design**: Mobile-first with TailwindCSS
- **Bloomberg-Inspired UI**: Professional financial news aesthetic
- **Optimized Performance**: React Query for caching and state management

---

## ğŸ›  Tech Stack

### Backend
| Technology | Purpose |
|------------|----------|
| **Python 3.11+** | Core programming language |
| **Django 5.0** | Web framework |
| **Django REST Framework** | API development |
| **Celery + Flower** | Async task processing with monitoring |
| **Redis** | Caching & message broker |
| **PostgreSQL 16** | Primary database with FTS & pg_trgm |
| **Whitenoise** | Static file serving |
| **Structlog** | Structured logging |
| **BeautifulSoup4** | Web scraping |
| **feedparser** | RSS feed parsing |

### Frontend
| Technology | Purpose |
|------------|---------|
| **Next.js 14** | React framework |
| **React 18** | UI library |
| **TypeScript 5.3** | Type safety |
| **TailwindCSS 3.4** | Styling |
| **React Query** | Server state management |
| **Axios** | HTTP client |
| **Lucide React** | Icons |

### Infrastructure
| Technology | Purpose |
|------------|---------|
| **Docker** | Containerization |
| **Docker Compose** | Orchestration |
| **Nginx** | Reverse proxy |
| **Gunicorn** | WSGI server |

---

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- Git

### 1. Clone the Repository
```bash
git clone https://github.com/ObaidOIS/TSFSE.git
cd TSFSE
```

### 2. Environment Setup
```bash
# Copy environment template
cp backend/.env.example backend/.env

# Edit .env with your settings
nano backend/.env
```

### 3. Start with Docker
```bash
# Build and start all services
docker-compose up -d

# View logs
docker-compose logs -f

# Run migrations
docker-compose exec backend python manage.py migrate

# Create superuser
docker-compose exec backend python manage.py createsuperuser
```

### 4. Access the Application
- **Frontend**: http://localhost:3000
- **Backend API**: http://127.0.0.1:8000/api/news/
- **Admin Panel**: http://127.0.0.1:8000/admin/
- **Flower Dashboard**: http://127.0.0.1:5555 (admin/bloomberg_flower)
- **Health Check**: http://127.0.0.1:8000/health/
- **API Docs**: http://127.0.0.1:8000/api/docs/

---

## ğŸ“– Local Development

### Backend Setup
```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
.\venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Set environment variables
export DATABASE_URL=postgres://user:pass@localhost:5432/bloomberg
export REDIS_URL=redis://localhost:6379/0

# Run migrations
python manage.py migrate

# Start development server
python manage.py runserver
```

### Frontend Setup
```bash
cd frontend

# Install dependencies
npm install

# Start development server
npm run dev
```

### Start Celery Workers
```bash
# In separate terminal windows:

# Celery worker
celery -A config worker -l INFO

# Celery beat (scheduler)
celery -A config beat -l INFO
```

---

## ğŸ“¡ API Reference

### Articles

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/articles/` | GET | List all articles |
| `/api/articles/{id}/` | GET | Get article details |
| `/api/articles/search/` | GET | Search articles |
| `/api/articles/categories/` | GET | List categories with counts |
| `/api/articles/latest/` | GET | Get latest articles |

### Search

```http
GET /api/articles/search/?q=economy&category=market&page=1
```

**Response:**
```json
{
  "total_results": 150,
  "page": 1,
  "page_size": 10,
  "total_pages": 15,
  "detected_category": "economy",
  "category_confidence": 0.87,
  "execution_time_ms": 45,
  "results": [...]
}
```

### Scraper Control

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/scraper/` | GET | Get scraper status |
| `/api/scraper/toggle/` | POST | Enable/disable scraper |
| `/api/scraper/trigger/` | POST | Manually trigger scrape |
| `/api/scraper/history/` | GET | View scraping history |

### Health & Monitoring

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health/` | GET | Health check endpoint |
| `/ready/` | GET | Readiness check |
| `/metrics/` | GET | Application metrics |
| `/metrics/prometheus/` | GET | Prometheus format metrics |

**Toggle Scraper:**
```http
POST /api/scraper/toggle/
Content-Type: application/json

{
  "fetch": true
}
```

---

## ğŸ— Architecture

### System Overview
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Client Browser                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Nginx (Reverse Proxy)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚                       â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Next.js Frontend    â”‚ â”‚  Django Backend   â”‚
          â”‚   (React, TypeScript) â”‚ â”‚  (REST API)       â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                                   â”‚                 â”‚
          â–¼                                   â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL    â”‚              â”‚     Redis       â”‚   â”‚  Celery Worker â”‚
â”‚   (Articles,    â”‚              â”‚  (Cache,        â”‚   â”‚  (Scraping,    â”‚
â”‚    Categories)  â”‚              â”‚   Broker)       â”‚   â”‚   AI Tasks)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                               â”‚
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚                      â”‚                      â”‚
                                        â–¼                      â–¼                      â–¼
                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                             â”‚  Celery Beat    â”‚    â”‚     Flower      â”‚    â”‚  Bloomberg RSS  â”‚
                             â”‚  (Scheduler)    â”‚    â”‚   (Monitoring)  â”‚    â”‚ (External Sourceâ”‚
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow
1. **Scraping**: Celery beat triggers periodic scraping â†’ Bloomberg RSS parsed â†’ Articles saved
2. **Categorization**: New articles â†’ AI categorizer â†’ Category assigned with confidence
3. **Indexing**: Article saved â†’ Search vector updated â†’ Full-text index refreshed
4. **Search**: Query received â†’ Category detected â†’ PostgreSQL FTS â†’ Results ranked & returned

---

## ğŸ“ Project Structure

```
bloomberg-news-scraper/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ apps/
â”‚   â”‚   â”œâ”€â”€ news/              # News articles app
â”‚   â”‚   â”‚   â”œâ”€â”€ models.py      # Article, Category models
â”‚   â”‚   â”‚   â”œâ”€â”€ views.py       # API endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ views_health.py # Health & metrics endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ serializers.py # DRF serializers
â”‚   â”‚   â”‚   â”œâ”€â”€ middleware.py  # Request correlation & monitoring
â”‚   â”‚   â”‚   â”œâ”€â”€ signals.py     # Django signals
â”‚   â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚   â”‚       â”œâ”€â”€ categorizer.py  # AI categorization
â”‚   â”‚   â”‚       â””â”€â”€ search.py       # Search engine
â”‚   â”‚   â””â”€â”€ scraper/           # Scraper app
â”‚   â”‚       â”œâ”€â”€ bloomberg_scraper.py
â”‚   â”‚       â”œâ”€â”€ tasks.py       # Celery tasks
â”‚   â”‚       â””â”€â”€ views.py       # Scraper control API
â”‚   â”œâ”€â”€ config/                # Django settings
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/              # Next.js App Router
â”‚   â”‚   â”œâ”€â”€ components/       # React components
â”‚   â”‚   â”œâ”€â”€ hooks/            # Custom hooks
â”‚   â”‚   â”œâ”€â”€ services/         # API services
â”‚   â”‚   â””â”€â”€ types/            # TypeScript types
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ nginx/
â”‚   â”œâ”€â”€ nginx.conf
â”‚   â””â”€â”€ conf.d/
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## ğŸ§ª Testing

### Backend Tests
```bash
cd backend

# Run all tests
python manage.py test

# Run with coverage
coverage run --source='.' manage.py test
coverage report
```

### Frontend Tests
```bash
cd frontend

# Run tests
npm test

# Run with coverage
npm run test:coverage
```

---

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|----------|
| `DEBUG` | Enable debug mode | `False` |
| `SECRET_KEY` | Django secret key | Required |
| `DATABASE_URL` | PostgreSQL connection | Required |
| `REDIS_URL` | Redis connection | Required |
| `CORS_ALLOWED_ORIGINS` | CORS whitelist | `http://localhost:3000` |
| `NEXT_PUBLIC_API_URL` | Backend API URL | `http://localhost:8000/api` |
| `FLOWER_USER` | Flower dashboard username | `admin` |
| `FLOWER_PASSWORD` | Flower dashboard password | `bloomberg_flower` |

---

## ğŸ“ˆ Performance

- **Search Response**: < 100ms average
- **Article Indexing**: Real-time with PostgreSQL triggers
- **Scraping Rate**: 100+ articles/minute with rate limiting
- **AI Categorization**: < 10ms per article (lightweight keyword-based)
- **Docker Image Size**: Optimized multi-stage builds

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¨â€ğŸ’» Author

**Obaidulllah**

Built with â¤ï¸ for the Technical Assessment

---

<div align="center">

**â­ Star this repo if you found it helpful!**

</div>
